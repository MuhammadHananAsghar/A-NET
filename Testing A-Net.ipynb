{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d46800ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import dlib\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54e70dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    \n",
    "    # Read in image from file path\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    # Load in the image \n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    \n",
    "    # Preprocessing steps - resizing the image to be 100x100x3\n",
    "    img = tf.image.resize(img, (96,96))\n",
    "    # Scale image to be between 0 and 1 \n",
    "    img = img / 255.0\n",
    "    \n",
    "    # Return image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a9ac0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TFLIte without OPTIMIZE\n",
    "interpreter = tf.lite.Interpreter(model_path=\"a_net.tflite\")\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52901607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1159376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'serving_default_validation_image:0',\n",
       "  'index': 0,\n",
       "  'shape': array([ 1, 96, 96,  3], dtype=int32),\n",
       "  'shape_signature': array([-1, 96, 96,  3], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'serving_default_input_image:0',\n",
       "  'index': 1,\n",
       "  'shape': array([ 1, 96, 96,  3], dtype=int32),\n",
       "  'shape_signature': array([-1, 96, 96,  3], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "792736a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'StatefulPartitionedCall:0',\n",
       "  'index': 38,\n",
       "  'shape': array([1, 1], dtype=int32),\n",
       "  'shape_signature': array([-1,  1], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d252432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['database/mohsin', 'database/ehsan', 'database/arham']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "database = glob.glob(\"database/*\")\n",
    "print(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2935f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arham': 0, 'ehsan': 2, 'mohsin': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {'arham': 0, 'ehsan': 2, 'mohsin': 1}\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cb8db19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arham Image\n",
    "# input_img = preprocess(os.path.join(\"dataset\", \"cropped\", \"arham\", \"9a1b3c24-99df-11ec-ab0e-842b2bb3a5b6.jpg\"))\n",
    "# input_img = preprocess(os.path.join(\"dataset\", \"cropped\", \"mohsin\", \"0a2bbaa0-99e0-11ec-826f-842b2bb3a5b6.jpg\"))\n",
    "input_img = preprocess(os.path.join(\"dataset\", \"cropped\", \"ehsan\", \"4a02d925-99e0-11ec-be8c-842b2bb3a5b6.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a32a6194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([96, 96, 3])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c59cb29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_mappings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a6cb7d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "database/mohsin/0a3cd26f-99e0-11ec-b772-842b2bb3a5b6.jpg\n",
      "database/ehsan/4a0dbcc7-99e0-11ec-aa89-842b2bb3a5b6.jpg\n",
      "database/arham/9a0cac9e-99df-11ec-8d31-842b2bb3a5b6.jpg\n"
     ]
    }
   ],
   "source": [
    "for database_dir in database:\n",
    "    label = database_dir.split(\"/\")[-1]\n",
    "    for validate_img in glob.glob(database_dir+\"/*\"):\n",
    "        print(validate_img)\n",
    "        validation_img = preprocess(validate_img)\n",
    "        validation_mappings.append([validation_img, label_map[label]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7104bccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_mappings = np.array(validation_mappings)\n",
    "validation_mappings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d518afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"embeddings\", validation_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d76a3a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = np.load(\"embeddings.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7bfc852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a77f43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Seconds: 1.62sec.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "results = [[], [], []]\n",
    "for validate in vp:\n",
    "    embs, label = validate\n",
    "    interpreter.set_tensor(input_details[0]['index'], np.expand_dims(input_img, axis=0))\n",
    "    interpreter.set_tensor(input_details[1]['index'], np.expand_dims(embs, axis=0))\n",
    "    interpreter.invoke()\n",
    "    pred = interpreter.get_tensor(output_details[0]['index'])\n",
    "    if label == 0:\n",
    "        results[0].append(pred)\n",
    "    if label == 1:\n",
    "        results[1].append(pred)\n",
    "    if label == 2:\n",
    "        results[2].append(pred)\n",
    "print(f\"Total Seconds: {round(time.time()-start, 2)}sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78e580ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ehsan\n"
     ]
    }
   ],
   "source": [
    "if np.argmax(results) == 0:\n",
    "    print(\"Arham\")\n",
    "if np.argmax(results) == 1:\n",
    "    print(\"Mohsin\")\n",
    "if np.argmax(results) == 2:\n",
    "    print(\"Ehsan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38629d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e690b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
